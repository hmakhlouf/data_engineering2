{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa3f256-5aa7-4cda-919b-d27e461a8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d075d8-5385-4b7c-b8e6-a3cb931d08c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 00:50:22 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/18 00:50:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/18 00:50:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/18 00:50:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"UDF\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "# in single spark driver/note book/spark application,\n",
    "# there can be many spark sessions, and \n",
    "# only one spark context\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .getOrCreate()\n",
    "# spark core operations, like rdd, partitions, actions etc\n",
    "# spark session shall use catalyst engine, which will use spark context for low level\n",
    "# code execution\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df0145b-4632-4a98-b968-68bb779df9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 00:50:27 WARN SimpleFunctionRegistry: The function power replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(n)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power = lambda n : n * n\n",
    "\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import LongType\n",
    "# create udf with return type\n",
    "powerUdf = udf(power, LongType())\n",
    "\n",
    "# we must register udf in spark session\n",
    "# udf too private within spark session, udf registered in spark session not avaialble in another spark session\n",
    "spark.udf.register(\"power\", powerUdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb43949b-6c7c-40ab-9ed5-406d4c92cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|power(5)|\n",
      "+--------+\n",
      "|      25|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# power is udf function\n",
    "spark.sql(\"SELECT power(5)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a935976-d489-405a-999a-11c2cd9e7a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-----+---+--------+----+\n",
      "|product_id|product_name|brand_id|price|qty|discount|taxp|\n",
      "+----------+------------+--------+-----+---+--------+----+\n",
      "|         1|      iPhone|     100| 1000|  2|       5|  18|\n",
      "|         2|      Galaxy|     200|  800|  1|       8|  22|\n",
      "+----------+------------+--------+-----+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "orders = [ \n",
    "          # (product_id, product_name, brand_id, price, qty, discount, taxp)  \n",
    "         (1, 'iPhone', 100, 1000, 2, 5, 18),\n",
    "         (2, 'Galaxy', 200, 800, 1, 8, 22),\n",
    "]\n",
    " \n",
    "orderDf = spark.createDataFrame(data=orders, schema=[\"product_id\", \"product_name\", \"brand_id\", \"price\", \"qty\", \"discount\", \"taxp\"])\n",
    "orderDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e99690e-ce65-4ca9-8ffc-8909c0052163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount is  2242.0\n",
      "2242.0\n"
     ]
    }
   ],
   "source": [
    "# UDF Function to calculate amount\n",
    "# amount = (price * qty) * apply discount * taxp\n",
    "def calculateAmount(price, qty, discount, taxp):\n",
    "    a = price * qty\n",
    "    a = a - ( a * discount/100) # discounted price\n",
    "    amount = a + a * taxp/100 #with tax\n",
    "    print(\"amount is \" , amount)\n",
    "    return amount \n",
    "\n",
    "\n",
    "print(calculateAmount(1000, 2, 5, 18))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e66a261-a54b-493a-9e62-cf785a74dd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.calculateAmount(price, qty, discount, taxp)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "# create UDF FUNCTION \n",
    "\n",
    "calculate = udf(calculateAmount, DoubleType())\n",
    "\n",
    "# \"calculate\" is used in spark sql SELECT calculate(...)\n",
    "spark.udf.register(\"cal\", calculate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f611941a-fd32-4aa4-86af-918ebc6d86a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- brand_id: long (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      " |-- taxp: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|product_id|product_name|brand_id|price|qty|discount|taxp|amount|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|         1|      iPhone|     100| 1000|  2|       5|  18|2242.0|\n",
      "|         2|      Galaxy|     200|  800|  1|       8|  22|897.92|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amount is  2242.0\n",
      "amount is  897.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use udf in data frame\n",
    "from pyspark.sql.functions import col\n",
    "df = orderDf.withColumn(\"amount\", calculate( col(\"price\"), col(\"qty\"), col(\"discount\"), col(\"taxp\")))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1dda4f-91a8-41df-96da-6cebb2dcbcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-----+---+--------+----+\n",
      "|product_id|product_name|brand_id|price|qty|discount|taxp|\n",
      "+----------+------------+--------+-----+---+--------+----+\n",
      "|         1|      iPhone|     100| 1000|  2|       5|  18|\n",
      "|         2|      Galaxy|     200|  800|  1|       8|  22|\n",
      "+----------+------------+--------+-----+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create temp table \n",
    "\n",
    "orderDf.createOrReplaceTempView(\"order\")\n",
    "\n",
    "\n",
    "spark.sql(\"select * from order\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea782fa-50cd-40a0-bb19-ecbeca044e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- brand_id: long (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      " |-- taxp: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|product_id|product_name|brand_id|price|qty|discount|taxp|amount|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|         1|      iPhone|     100| 1000|  2|       5|  18|2242.0|\n",
      "|         2|      Galaxy|     200|  800|  1|       8|  22|897.92|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amount is  2242.0\n",
      "amount is  897.92\n"
     ]
    }
   ],
   "source": [
    "# now apply udf on spark sql\n",
    "\n",
    "df = spark.sql(\"SELECT *, cal(price, qty, discount, taxp) as amount from order\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638aff7-af2b-45f2-ba94-8826bb02a1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
