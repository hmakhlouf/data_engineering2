{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a230b7-f2a5-46a2-b999-75d51b3de709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spark sql with spark local database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c97d2c-6bac-4f53-b65b-e732ea8f1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example. using local meta store, using local  to store data\n",
    "# meta store is not usable in other notebooks, as it is embedeed locked jvm internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270cdba8-7cd8-4f25-ba2f-d7f676cd3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18e54e9-732e-421b-8be0-fd8cb963c34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark Database \\nBasic Database:\\nSingle , local database, not shared across multiple notebooks,\\nnot shared across worker, in  built into spark working director,\\ngenerally used for simple development/learning purpose\\n\\nIn production, you will be using Hive Data Catalog\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Database \n",
    "Basic Database:\n",
    "Single , local database, not shared across multiple notebooks,\n",
    "not shared across worker, in  built into spark working director,\n",
    "generally used for simple development/learning purpose\n",
    "\n",
    "In production, you will be using Hive Data Catalog\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72ad730-4318-4992-9279-7b3c46e2f5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark Local Database\\nOnly for dev only, not for production\\n\\n3 components involved\\n\\n1. meta data - database name, tables, columns data types, location where data stored\\n    is managed by hive, hive internally uses derby db to store all meta data\\n2. spark temporary location  \"spark.local.dir\", \"/home/ubuntu/spark-temp\"\\n    where temp data used internally stored\\n    \\n3. \"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehouse\" spark data warehouse where all the database data shall be stored\\n    we can see database, tables, their data where meta data ,table name, columns are stored in \\n    meta data\\n    \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Local Database\n",
    "Only for dev only, not for production\n",
    "\n",
    "3 components involved\n",
    "\n",
    "1. meta data - database name, tables, columns data types, location where data stored\n",
    "    is managed by hive, hive internally uses derby db to store all meta data\n",
    "2. spark temporary location  \"spark.local.dir\", \"/home/ubuntu/spark-temp\"\n",
    "    where temp data used internally stored\n",
    "    \n",
    "3. \"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehouse\" spark data warehouse where all the database data shall be stored\n",
    "    we can see database, tables, their data where meta data ,table name, columns are stored in \n",
    "    meta data\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5f5035-2b0e-49f4-9048-f55e896bfc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:13:28 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/16 19:13:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/16 19:13:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/16 19:13:30 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"SparkDatabase\")\n",
    "\n",
    "# embedded, simple, local spark database/warehouse\n",
    "# spark will store temporary files\n",
    "# enable hive support must for sql database\n",
    "# enable hiveSupport hive catalog to be embedded inside working directory\n",
    "# spark temp data goes to \"/home/ubuntu/spark-temp\"\n",
    "config.set(\"spark.local.dir\", \"/home/ubuntu/spark-temp\")\n",
    "# spark data [not meta data] goes into  \"/home/ubuntu/spark-warehouse\"\n",
    "config.set(\"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehouse\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "\n",
    "# enableHiveSupport() create a meta catalog/database using derby database\n",
    "# inside current working directory, embedded into spark notebook,\n",
    "# multiple notebooks cannot share at same time.\n",
    "# inside pyspark-notebooks, you could see metastore_db\n",
    "# metastore shall have meta data: database, tables, columns, data types, where exactly\n",
    "# data located in hdfs or file system or s3\n",
    "# derby.log - derby database log \n",
    "## metastore_db \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19334e06-8c78-4192-b329-4f096615f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:18:50 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/05/16 19:18:50 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/05/16 19:18:55 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/05/16 19:18:55 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore ubuntu@127.0.1.1\n",
      "22/05/16 19:18:55 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SHOW DATABASES\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097a52ad-9910-4903-bab1-35b89d130a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:24:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/05/16 19:24:48 WARN ObjectStore: Failed to get database stocklocaldb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS stocklocaldb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1881e7c2-f324-4efe-9872-7cdc01e24a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:26:56 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "22/05/16 19:26:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "22/05/16 19:26:56 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "22/05/16 19:26:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/05/16 19:26:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/05/16 19:26:56 WARN HiveMetaStore: Location: file:/home/ubuntu/spark-warehouse/stocklocaldb.db/stocks specified for non-external table:stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spark Managed table\n",
    "# we have to use spark sql like insert, (update, delete won't work at 2.x)\n",
    "# to add data\n",
    "spark.sql(\"CREATE TABLE  IF NOT EXISTS stocklocaldb.stocks(symbol STRING, industry STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b76b0fe2-ec44-44b5-8398-d591d3bdc4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|stocklocaldb|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576e3fb3-ca38-4796-ab72-fa444a8722bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|    database|tableName|isTemporary|\n",
      "+------------+---------+-----------+\n",
      "|stocklocaldb|   stocks|      false|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN stocklocaldb\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f6f326-dec2-4d0d-ae43-cc419b545fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    " INSERT INTO stocklocaldb.stocks VALUES('INFY', 'IT')\n",
    "\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94d85fc1-7cc2-41b9-9a54-671301bcd64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|symbol|industry|\n",
      "+------+--------+\n",
      "|  INFY|      IT|\n",
      "|  INFY|      IT|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM stocklocaldb.stocks\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a643400-e5a5-4c7f-a35c-095ede7499b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command drop the table from meta data store and drop the in the \n",
    "# spark datawarehouse directory\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS stocklocaldb.stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e467a625-741f-44f1-bb19-fc1fab86675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f20625b-394d-4aeb-a9d8-0e6c030c3630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:55:41 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if no table exists, no data inside, then it drop the database\n",
    "# drop the metadata too..\n",
    "spark.sql(\"DROP DATABASE IF EXISTS stocklocaldb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2b666d0-aaf2-4e65-a84c-e9fbd58549d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f2c8c8a-efc8-4da8-8d6a-a3511763cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:59:19 WARN ObjectStore: Failed to get database productdb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create databsae called productdb\n",
    "# /home/ubuntu/spark-warehouse/productdb.db\n",
    "spark.sql(\"\"\"CREATE DATABASE IF NOT EXISTS productdb\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e4c07ab-287f-4e90-ad3b-b8aace69b235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 19:59:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "22/05/16 19:59:57 WARN HiveMetaStore: Location: file:/home/ubuntu/spark-warehouse/productdb.db/products specified for non-external table:products\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a table called products with 3 fields (id int, name string, price int)\n",
    "# create table productdb.products ...\n",
    "# create in ubuntu /home/ubuntu/spark-warehouse/productdb.db/products\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS \n",
    "    productdb.products( id INT, \n",
    "                        product_name STRING, \n",
    "                        price INT )\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe75f0fc-3f1e-4a9d-8ef7-3c1b4a222e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert at least 2 records productdb.products\n",
    "# created in ubuntu /home/ubuntu/spark-warehouse/productdb.db/products/part*\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO productdb.products VALUES(10, 'Cheese', 3), \n",
    "                                        (25, 'Apricot', 2)\n",
    "\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d82697fd-f333-47d5-975d-0c1fb9ea14db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----+\n",
      "| id|product_name|price|\n",
      "+---+------------+-----+\n",
      "| 10|      Cheese|    3|\n",
      "| 25|     Apricot|    2|\n",
      "+---+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query data using productdb.produces \n",
    "spark.sql(\"SELECT * FROM productdb.products\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54705744-9a91-44f7-b105-869e53e0fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
