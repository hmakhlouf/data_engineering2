{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c78e140-cdf9-4580-93b2-7c868e1f2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark\n",
    "# finds spark installation in the system, \n",
    "# automatically set path, environment needed for pyspark\n",
    "# load spark libraries etc\n",
    "# /opt/spark-3.1.2.......\n",
    "# good for single machine development, learning\n",
    "\n",
    "import findspark \n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c5edc0-8adb-48d4-a275-1bc4b4b24c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 00:01:57 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/05 00:01:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/05 00:01:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/05 00:02:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Context - helps to create rdd, dag, job, task execute task etc, spark context is SPARK CORE \n",
    "# this code is called spark application, or spark driver\n",
    "# every spark driver shall have ONLY ONE spark context\n",
    "from pyspark import SparkContext\n",
    "# local is execution mode, spark driver, \n",
    "# spark executor runs in same JVM in same machine / not distributed\n",
    "# good for development, learning , not for production\n",
    "# SparkBasic is application name of your choice\n",
    "sc = SparkContext(\"local\", \"SparkBasic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d63a3a-3bfa-4c2c-9755-57fc25b1730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD, from hardcoded data\n",
    "# data is hardcoded in Spark Driver, this notebook\n",
    "# RDD shall be created in Executor process\n",
    "# Lazy evaluation \n",
    "# intellisense - editor/notebook automatically brings up functions as you type\n",
    "# sc.<TAB><TAB><TAB> - will bring up all functions\n",
    "# creating RDD using parallelize method, by loading hardcoded data\n",
    "# RDD shall have partition(s)\n",
    "# the data hardcoded shall be loaded into partitions\n",
    "# at this moment, no data shall be loaded, as this is lazy loading\n",
    "# when we apply action only then data shall be loaded\n",
    "# create and return \n",
    "rdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fa0e49-7f0d-493a-8158-c18e21a9424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter operation, we call this as TRANSFORMATION c\n",
    "# TRANSFORMATION is code/task applied on partitioned data - check notes for may 4th @ rdd low level core of spark slide \n",
    "# filter is higher order function, accept a function as input\n",
    "# filter apply data (n) from partition to function supplied lambda n: n % 2 == 1\n",
    "# lambda n: n % 2 == 1 returns either true or false,\n",
    "# filter collect all the numbers where fitler return true 1, 3, 5, 7, 9\n",
    "# LAZY Evaluation, no partition, no data , not code loaded into Exeuctor\n",
    "# until we apply ACTION on RDD\n",
    "# lamda n: n % 2 == 1 shall be executed by executor\n",
    "# spark driver shall send lambda n: n % 2 == 1 to excuter called task\n",
    "\n",
    "oddRdd = rdd.filter (lambda n: n % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad82905-3198-433b-a1cb-f1b11cb79842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# collect is an ACTION method\n",
    "# Every ACTION Methods create JOB\n",
    "# JOB is split into STAGES\n",
    "# Each STAGE shall have TASKs\n",
    "# TASKs shall be running on Executor on PARTITION\n",
    "# Finally, collect bring the output back to DRIVER from EXECUTORs\n",
    "# Action is the one will create and distribute partitions, run tasks on executors\n",
    " \n",
    "results = oddRdd.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54205f56-429b-4595-8e95-98d140e4fa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# min is an ACTION,\n",
    "# this create job, stages, DAG, tasks execute on cluster independently \n",
    "r = oddRdd.min ()\n",
    "print(r) # print min of odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca42df70-55af-42f4-b89c-cebffe0f6c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "r2 = oddRdd.max()\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0de5aa53-c6e4-4ec8-a18a-d638cb10aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "r3 = oddRdd.mean()\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffba873e-916c-4edd-ad6e-c36a3a908176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "r4 = oddRdd.sum()\n",
    "print(r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2caac22-87f2-4fb5-956c-d4ff9001484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- create rdd \n",
    "# 2- tranform \n",
    "# 3- perform actions \n",
    "# 4- results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8913e0-d8c5-453e-8fd7-27140e13ead8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
