{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6649a1-06ab-4cf3-9cc4-60b426937770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f45bc7-4af4-4cdc-b644-cfa66c2d25b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/11 08:35:17 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/11 08:35:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/11 08:35:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "22/05/11 08:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "22/05/11 08:35:20 ERROR SparkUI: Failed to bind SparkUI\n",
      "java.net.BindException: Failed to bind to /0.0.0.0:4056: Service 'SparkUI' failed after 16 retries (starting from 4040)! Consider explicitly setting the appropriate port for the service 'SparkUI' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.\n",
      "\tat org.sparkproject.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:349)\n",
      "\tat org.sparkproject.jetty.server.ServerConnector.open(ServerConnector.java:310)\n",
      "\tat org.sparkproject.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)\n",
      "\tat org.sparkproject.jetty.server.ServerConnector.doStart(ServerConnector.java:234)\n",
      "\tat org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\n",
      "\tat org.apache.spark.ui.JettyUtils$.newConnector$1(JettyUtils.scala:302)\n",
      "\tat org.apache.spark.ui.JettyUtils$.httpConnect$1(JettyUtils.scala:333)\n",
      "\tat org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$5(JettyUtils.scala:336)\n",
      "\tat org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$5$adapted(JettyUtils.scala:336)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$startServiceOnPort$2(Utils.scala:2331)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2323)\n",
      "\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:337)\n",
      "\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:146)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$11(SparkContext.scala:486)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$11$adapted(SparkContext.scala:486)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:486)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1211, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41599)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_70806/947502926.py\", line 2, in <cell line: 2>\n",
      "    sc = SparkContext(\"local\", \"MoviesTuple\")\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 146, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 209, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 321, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41599)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_70806/947502926.py\", line 2, in <cell line: 2>\n",
      "    sc = SparkContext(\"local\", \"MoviesTuple\")\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 146, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 209, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 321, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41599)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_70806/947502926.py\", line 2, in <cell line: 2>\n",
      "    sc = SparkContext(\"local\", \"MoviesTuple\")\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 146, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 209, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 321, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41599)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_70806/947502926.py\", line 2, in <cell line: 2>\n",
      "    sc = SparkContext(\"local\", \"MoviesTuple\")\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 146, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 209, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 321, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41599)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_70806/947502926.py\", line 2, in <cell line: 2>\n",
      "    sc = SparkContext(\"local\", \"MoviesTuple\")\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 146, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 209, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 321, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41599)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_70806/947502926.py\", line 2, in <cell line: 2>\n",
      "    sc = SparkContext(\"local\", \"MoviesTuple\")\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 146, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 209, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 321, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41599)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_70806/947502926.py\", line 2, in <cell line: 2>\n",
      "    sc = SparkContext(\"local\", \"MoviesTuple\")\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 146, in __init__\n",
      "    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 209, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py\", line 321, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1568, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMoviesTuple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/context.py:321\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, jconf):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1568\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1562\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1564\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1565\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1567\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1568\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1572\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"MoviesTuple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc80c441-5515-4073-a7ca-a0460fecc9ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# read a text file from HDFS, movies.csv\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# hdfs dfs -ls /ml-latest-small\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://localhost:9000/ml-latest-small/movies.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# read a text file from HDFS, movies.csv\n",
    "# hdfs dfs -ls /ml-latest-small\n",
    "rdd = sc.textFile(\"hdfs://localhost:9000/ml-latest-small/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25b1e3-d1d4-4bf9-987a-c0adf489a765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
