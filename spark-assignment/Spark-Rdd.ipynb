{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/12 20:37:27 WARN Utils: Your hostname, Hocines-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.164 instead (on interface en0)\n",
      "22/05/12 20:37:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/12 20:37:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/12 20:37:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/12 20:37:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/05/12 20:37:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"ReadFromHDFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/12 20:37:35 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3025)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:826)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:751)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'movieId,title,genres'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 - Load movies data into spark rdd tuple with (movieid, title, genres) format\n",
    "\n",
    "rddMovies = sc.textFile(\"hdfs://localhost:9000/ml-latest-small/movies.csv\")\n",
    "\n",
    "# get first line\n",
    "rddMovies.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9743"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 - Count number of movies based on Genres fields, \n",
    "# use Adventure, Romance, Comedy as specific genres, get the count of movies on each genres\n",
    "\n",
    "rddMovies.count()\n",
    "\n",
    "\n",
    "#wordCountRdd = sc.textFile(\"hdfs://localhost:9000/books/book-war-and-peace.txt\")\\\n",
    "                # .map (lambda line: to_ascii(line))\\\n",
    "               #   .map (lambda line: line.strip().lower())\\\n",
    "                # .map (lambda line: line.split(\" \"))\\\n",
    "               #  .flatMap(lambda elements: elements)\\\n",
    "               #  .filter (lambda word: word != \"\")\\\n",
    "               #  .map (lambda word: (word, 1))\\\n",
    "                # .reduceByKey(lambda acc, value: acc + value)\n",
    "\n",
    "#wordCountRdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/12 20:38:54 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3025)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:826)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:751)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'userId,movieId,rating,timestamp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 - Load ratings.csv into tuple (userid, movieid, rating, timestamp) format\n",
    "\n",
    "rddRatings = sc.textFile(\"hdfs://localhost:9000/ml-latest-small/ratings.csv\")\n",
    "\n",
    "# get first line\n",
    "rddRatings.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Remove the rating which is lower than 1.0 from the ratings.csv\n",
    "\n",
    "# use moviesRdd2 and filter, print the movies id  betwen 100 to 105\n",
    "filteredRdd = moviesRdd2.filter (lambda t: t[0] >= 100 and t[0] <= 105)\n",
    "print(filteredRdd.take(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to skip first line, \n",
    "headerLine = rdd.first()\n",
    "# no header data in rddContent\n",
    "rddContent = rdd.filter (lambda line: line != headerLine)\n",
    "\n",
    "print (rddContent.take(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Use reduceByKey, count the ratings for the movies based on movieid, for example, if a movie received 10 ratings, \n",
    "#the output should be a tuple (movieId, 10)\n",
    "\n",
    "\n",
    "# convert word into (key,value) rdd (spark, 1) for reduceByKey\n",
    "pairRdd = wordRdd.map (lambda word: (word, 1))\n",
    "pairRdd.take(5)\n",
    "\n",
    "# get word count using reduceByKey\n",
    "# transformation\n",
    "wordCountRdd = pairRdd.reduceByKey(lambda acc, value: acc + value)\n",
    "wordCountRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the result into text file in hdfs\n",
    "# saveAsTextFile is an ACTION Method\n",
    "# word-count-results is a folder, inside we will shall partition files\n",
    "\n",
    "wordCountRdd.saveAsTextFile (\"hdfs://localhost:9000/word-count-results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check flod file on github \n",
    "\n",
    "# 6 - Use Fold or FoldByKey, find the sum/or avg rating received by a movie in rating.csv\n",
    "\n",
    "\n",
    "# fold with aggregate with start value 0\n",
    "# fold is action method\n",
    "# fold works with each partition first, calculate add function on each partition\n",
    "# + it will apply result of all paritions into again another folder\n",
    "# return value of add is passed as input with next number in seq\n",
    "\n",
    "# after processing data from partition 0, it got result 150\n",
    "# then it will apply add function across partition result  acc 0 value 150\n",
    "result = rdd.fold (0, add) # output shall be 150 = 0 + 10 + 20 + 30 + 40 + 50\n",
    "print(\"result\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Convert timestamp in rating.csv into Python date time object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check s011 and s012\n",
    "\n",
    "# 8 - Join ratings and movies based on movie id, so that we could see movie title and genres for the given movie\n",
    "\n",
    "\n",
    "# joins ,\n",
    "\n",
    "sectors = [\n",
    "    # tuple, the key is used for join purpose\n",
    "    # sector id, sector name\n",
    "    (0, \"AUTOMOTIVE\"),\n",
    "    (1, \"TEXTILE\"),\n",
    "    (2, \"IT\"),\n",
    "    (4, \"MANUFACTURING\")\n",
    "]\n",
    "\n",
    "stocks = [\n",
    "    # key   is used for join purpose\n",
    "    \n",
    "    # key is 0, 1, 4 and value is  (\"TSLA\", 100), SYM1, SYM2\n",
    "    # sector id, symbol, price\n",
    "    (0, (\"TSLA\", 100) ),\n",
    "    (0, (\"GM\", 40 ) ),\n",
    "    (1, (\"SYM1\", 45) ),\n",
    "    (4, (\"SYM2\", 67) )\n",
    "]\n",
    "\n",
    "sectorRdd = sc.parallelize(sectors)\n",
    "stocksRdd = sc.parallelize(stocks)\n",
    "# join based on rdd keys it has to match sectorRdd sector id with stocksRdd sector id\n",
    "joinRdd = sectorRdd.join(stocksRdd)\n",
    "\n",
    "joinRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check from dataframe s52\n",
    "\n",
    "# 9 - Use sort on question number 5, sort the movie rating count by descending order so that highest rated movie should be on the top\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "rddSort  = rdd5.sort(desc(\"price\"))\n",
    "\n",
    "\n",
    "# alternatively use dataframe columns if we have df reference\n",
    "df = productDf.sort (productDf.price.asc())\n",
    "df.show()\n",
    "# desc\n",
    "df = productDf.sort (productDf.price.desc())\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check s008 s009\n",
    "\n",
    "\n",
    "# to skip first line, \n",
    "headerLine = rdd.first()\n",
    "# no header data in rddContent\n",
    "rddContent = rdd.filter (lambda line: line != headerLine)\n",
    "\n",
    "print (rddContent.take(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
