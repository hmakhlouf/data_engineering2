{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa17ee8-6dd0-4fb8-ac80-7de6e3d194ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark // spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3f05a-4f1e-4a7e-ba9a-58ee39efc7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5546f455-8c7c-4aef-a6b1-28d08aa8765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.174.129:4040\n",
       "SparkContext available as 'sc' (version = 3.1.3, master = local[*], app id = local-1652298188494)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined class Movie\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// case class\n",
    "case class Movie(movieId: Int, title: String, genre: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "885b38b6-12a2-4785-868e-29907fabb027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviesFileRdd: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/ml-latest-small/movies.csv MapPartitionsRDD[9] at textFile at <console>:25\n",
       "header: String = movieId,title,genres\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesFileRdd = sc.textFile(\"hdfs://localhost:9000/ml-latest-small/movies.csv\")\n",
    "val header = moviesFileRdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e6478-c24b-4956-8d02-6e09c65eecb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d841c53-9bbe-42b1-bf18-12c8134357a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkDriverExecutionException",
     "evalue": " Execution error",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkDriverExecutionException: Execution error",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1696)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2487)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.take(RDD.scala:1422)",
      "  at org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1463)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.first(RDD.scala:1463)",
      "  ... 31 elided",
      "Caused by: java.lang.ArrayStoreException: [LMovie;",
      "  at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:75)",
      "  at org.apache.spark.SparkContext.$anonfun$runJob$4(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.$anonfun$runJob$4$adapted(SparkContext.scala:2217)",
      "  at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:59)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1692)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2487)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      ""
     ]
    }
   ],
   "source": [
    "// case class\n",
    "case class Movie(movieId: Int, title: String, genres: String)\n",
    "val moviesRdd = (moviesFileRdd.filter( line => line != header) // remove first line\n",
    "                             .map (line => line.split(\",\")) // returns array of string\n",
    "                             .map(arr => Movie(arr(0).toInt, arr(1), arr(2))))   \n",
    "moviesRdd.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d649eb1-c347-4517-afbb-d4b790ddb063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
